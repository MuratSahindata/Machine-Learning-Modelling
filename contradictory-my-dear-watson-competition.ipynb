{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Doğru dosya yolları\ntrain_data_path = r'C:\\Users\\Lenovo\\Desktop\\Murat\\Kaggle Competition\\Contradictory, My Dear Watson\\train.csv'\ntest_data_path = r'C:\\Users\\Lenovo\\Desktop\\Murat\\Kaggle Competition\\Contradictory, My Dear Watson\\test.csv'\n\n# Verileri pandas ile yükleme\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\n\n# Eğitim ve test verilerine genel bakış\nprint(\"Training Data Overview:\")\nprint(train_data.head())  # Eğitim verilerinin ilk birkaç satırını gösterir\nprint(\"\\nTest Data Overview:\")\nprint(test_data.head())  # Test verilerinin ilk birkaç satırını gösterir\n\n# Eğitim verilerindeki eksik değerleri kontrol etme\nprint(\"\\nMissing values in training data:\")\nprint(train_data.isnull().sum())  # Eğitim setindeki eksik verileri gösterir\n\n# Test verilerindeki eksik değerleri kontrol etme\nprint(\"\\nMissing values in test data:\")\nprint(test_data.isnull().sum())  # Test setindeki eksik verileri gösterir\n","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Training Data Overview:\n\n           id                                            premise  \\\n\n0  5130fd2cb5  and these comments were considered in formulat...   \n\n1  5b72532a0b  These are issues that we wrestle with in pract...   \n\n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n\n3  5622f0c60b  you know they can't really defend themselves l...   \n\n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n\n\n                                          hypothesis lang_abv language  label  \n\n0  The rules developed in the interim were put to...       en  English      0  \n\n1  Practice groups are not permitted to work on t...       en  English      2  \n\n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n\n3  They can't defend themselves because of their ...       en  English      0  \n\n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  \n\n\n\nTest Data Overview:\n\n           id                                            premise  \\\n\n0  c6d58c3f69  بکس، کیسی، راہیل، یسعیاہ، کیلی، کیلی، اور کولم...   \n\n1  cefcc82292                             هذا هو ما تم نصحنا به.   \n\n2  e98005252c  et cela est en grande partie dû au fait que le...   \n\n3  58518c10ba                   与城市及其他公民及社区组织代表就IMA的艺术发展进行对话&amp   \n\n4  c32b0d16df                              Она все еще была там.   \n\n\n\n                                          hypothesis lang_abv language  \n\n0  کیسی کے لئے کوئی یادگار نہیں ہوگا, کولمین ہائی...       ur     Urdu  \n\n1  عندما يتم إخبارهم بما يجب عليهم فعله ، فشلت ال...       ar   Arabic  \n\n2                             Les mères se droguent.       fr   French  \n\n3                            IMA与其他组织合作，因为它们都依靠共享资金。       zh  Chinese  \n\n4     Мы думали, что она ушла, однако, она осталась.       ru  Russian  \n\n\n\nMissing values in training data:\n\nid            0\n\npremise       0\n\nhypothesis    0\n\nlang_abv      0\n\nlanguage      0\n\nlabel         0\n\ndtype: int64\n\n\n\nMissing values in test data:\n\nid            0\n\npremise       0\n\nhypothesis    0\n\nlang_abv      0\n\nlanguage      0\n\ndtype: int64\n"}]},{"cell_type":"code","source":"# Eksik verileri kontrol etme\nprint(train_data.isnull().sum())\n","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"id            0\n\npremise       0\n\nhypothesis    0\n\nlang_abv      0\n\nlanguage      0\n\nlabel         0\n\ndtype: int64\n"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Kategorik sütunları sayısal değerlere dönüştürme\nlabel_encoder = LabelEncoder()\n\n# Örneğin 'lang_abv' ve 'language' sütunlarını dönüştürelim\ntrain_data['lang_abv'] = label_encoder.fit_transform(train_data['lang_abv'])\ntest_data['lang_abv'] = label_encoder.transform(test_data['lang_abv'])\n\ntrain_data['language'] = label_encoder.fit_transform(train_data['language'])\ntest_data['language'] = label_encoder.transform(test_data['language'])\n","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Özellikler (X) ve hedef değişkeni (y) ayırma\nX_train = train_data.drop(columns=['label', 'id'])  # 'label' ve 'id' sütunlarını çıkarıyoruz\ny_train = train_data['label']  # Hedef değişken\n\n# Test seti için özellikler\nX_test = test_data.drop(columns=['id'])\n","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\n\n# TF-IDF vektörleştirme için ayrı ayrı vektörleştirici oluşturma\ntfidf_premise = TfidfVectorizer(max_features=5000)  # Premise için TF-IDF\ntfidf_hypothesis = TfidfVectorizer(max_features=5000)  # Hypothesis için TF-IDF\n\n# Premise ve hypothesis sütunlarını TF-IDF ile vektörleştiriyoruz\nX_train_premise = tfidf_premise.fit_transform(train_data['premise'])\nX_train_hypothesis = tfidf_hypothesis.fit_transform(train_data['hypothesis'])\n\nX_test_premise = tfidf_premise.transform(test_data['premise'])\nX_test_hypothesis = tfidf_hypothesis.transform(test_data['hypothesis'])\n\n# Vektörleştirilmiş premise ve hypothesis verilerini birleştirme\nfrom scipy.sparse import hstack\n\nX_train_combined = hstack([X_train_premise, X_train_hypothesis])\nX_test_combined = hstack([X_test_premise, X_test_hypothesis])\n\n# Hedef değişken (label)\ny_train = train_data['label']\n","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Eğitim ve doğrulama setlerine ayırma\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train_combined, y_train, test_size=0.2, random_state=42)\n\n# Logistic Regression modelini eğitme\nlog_reg = LogisticRegression(max_iter=200, random_state=42)\nlog_reg.fit(X_train_split, y_train_split)\n\n# Doğrulama seti üzerinde tahmin yapma\ny_pred = log_reg.predict(X_val)\n\n# Doğruluk skorunu hesaplama\naccuracy = accuracy_score(y_val, y_pred)\nprint(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"Validation Accuracy: 36.06%\n"}]},{"cell_type":"code","source":"# Test seti üzerinde tahmin yapma\ntest_predictions = log_reg.predict(X_test_combined)\n\n# Sonuç dosyasını oluşturma\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'prediction': test_predictions\n})\n\n# Sonuçları submission.csv dosyasına kaydetme\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")\n","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"Submission file created successfully!\n"}]},{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n","metadata":{},"execution_count":24,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting transformers\n\n  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n\n     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n\n     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n\n     ----------------- -------------------- 20.5/44.4 kB 330.3 kB/s eta 0:00:01\n\n     ----------------------------------- -- 41.0/44.4 kB 326.8 kB/s eta 0:00:01\n\n     -------------------------------------- 44.4/44.4 kB 310.3 kB/s eta 0:00:00\n\nRequirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n\nCollecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n\n  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n\nRequirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n\nRequirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n\nRequirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n\nCollecting safetensors>=0.4.1 (from transformers)\n\n  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n\n  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n\nRequirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n\nRequirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n\nRequirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n\nRequirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n\nDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n\n   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n\n   ---------------------------------------- 0.0/9.9 MB 2.0 MB/s eta 0:00:05\n\n    --------------------------------------- 0.1/9.9 MB 2.1 MB/s eta 0:00:05\n\n   - -------------------------------------- 0.3/9.9 MB 3.0 MB/s eta 0:00:04\n\n   -- ------------------------------------- 0.7/9.9 MB 4.5 MB/s eta 0:00:03\n\n   ------ --------------------------------- 1.5/9.9 MB 6.9 MB/s eta 0:00:02\n\n   --------- ------------------------------ 2.3/9.9 MB 8.6 MB/s eta 0:00:01\n\n   ------------- -------------------------- 3.4/9.9 MB 12.1 MB/s eta 0:00:01\n\n   ------------------ --------------------- 4.5/9.9 MB 12.4 MB/s eta 0:00:01\n\n   ------------------- -------------------- 4.9/9.9 MB 13.1 MB/s eta 0:00:01\n\n   -------------------------------- ------- 8.2/9.9 MB 18.6 MB/s eta 0:00:01\n\n   ------------------------------------- -- 9.2/9.9 MB 18.3 MB/s eta 0:00:01\n\n   ---------------------------------------- 9.9/9.9 MB 18.6 MB/s eta 0:00:00\n\nDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n\n   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n\n   --------------------------------------- 436.4/436.4 kB 26.6 MB/s eta 0:00:00\n\nDownloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n\n   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n\n   --------------------------------------- 286.3/286.3 kB 18.4 MB/s eta 0:00:00\n\nDownloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n\n   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n\n   --------------------------- ------------ 1.6/2.3 MB 33.6 MB/s eta 0:00:01\n\n   ---------------------------------------- 2.3/2.3 MB 29.5 MB/s eta 0:00:00\n\nInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n\nSuccessfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.20.0 transformers-4.45.1\n\nRequirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.4.1)\n\nRequirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n\nRequirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n\nRequirement already satisfied: sympy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (1.12)\n\nRequirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n\nRequirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n\nRequirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n\nRequirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n\nRequirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# PyTorch için veri kümesi sınıfı tanımlama\nclass NLIDataset(Dataset):\n    def __init__(self, premises, hypotheses, labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.premises)\n\n    def __getitem__(self, index):\n        premise = self.premises[index]\n        hypothesis = self.hypotheses[index]\n        label = self.labels[index]\n\n        # Premise ve Hypothesis metinlerini BERT için tokenleştirme\n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'premise': premise,\n            'hypothesis': hypothesis,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# BERT tokenizer'ı başlatma\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Modeli başlatma\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n# Eğitim ve doğrulama setine ayırma\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    list(zip(train_data['premise'], train_data['hypothesis'])),\n    train_data['label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Dataset oluşturma\ntrain_dataset = NLIDataset([x[0] for x in X_train_split], [x[1] for x in X_train_split], y_train_split, tokenizer, max_len=128)\nval_dataset = NLIDataset([x[0] for x in X_val], [x[1] for x in X_val], y_val, tokenizer, max_len=128)\n\n# DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Modeli eğitme fonksiyonu\ndef train_epoch(model, data_loader, optimizer, device):\n    model = model.train()\n    total_correct = 0\n    total_samples = 0\n\n    for batch in data_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        _, preds = torch.max(logits, dim=1)\n        total_correct += torch.sum(preds == labels)\n        total_samples += len(labels)\n\n        loss.backward()\n        optimizer.step()\n\n    accuracy = total_correct.double() / total_samples\n    return accuracy\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# Eğitim\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nfor epoch in range(3):\n    train_acc = train_epoch(model, train_loader, optimizer, device)\n    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_acc:.4f}\")\n","metadata":{},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26b81f0fe13b439b99ca14c254566e1c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n\n  warnings.warn(message)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6427bf70dd2b421e9ab634fcc8e22555","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"914d9eeb8a9144d6ae53a59146de8aff","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65e40296c7f04748b3aacf050bbeecdc","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e2ece0dd39b4634bae7be40cf5cdf28","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"},{"ename":"KeyError","evalue":"4299","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 4299","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 104\u001b[0m\n\u001b[0;32m    101\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m--> 104\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, device)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     72\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     77\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[26], line 22\u001b[0m, in \u001b[0;36mNLIDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m premise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpremises[index]\n\u001b[0;32m     21\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypotheses[index]\n\u001b[1;32m---> 22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Premise ve Hypothesis metinlerini BERT için tokenleştirme\u001b[39;00m\n\u001b[0;32m     25\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     26\u001b[0m     premise,\n\u001b[0;32m     27\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m )\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 4299"]}]},{"cell_type":"code","source":"# Eğitim ve doğrulama veri setlerindeki indeksleri sıfırlayın\ntrain_data = train_data.reset_index(drop=True)\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    list(zip(train_data['premise'], train_data['hypothesis'])),\n    train_data['label'],\n    test_size=0.2,\n    random_state=42\n)\n","metadata":{},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class NLIDataset(Dataset):\n    def __getitem__(self, index):\n        try:\n            premise = self.premises[index]\n            hypothesis = self.hypotheses[index]\n            label = self.labels[index]\n        except IndexError:\n            print(f\"Index {index} out of range for premises or labels\")\n            raise\n\n        # Tokenleştirme işlemi\n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'premise': premise,\n            'hypothesis': hypothesis,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Eğitim ve doğrulama veri setlerine ayırırken indeks sıfırlaması ekleyin\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    list(zip(train_data['premise'], train_data['hypothesis'])),\n    train_data['label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Bu işlemi yaptıktan sonra, her bir split'te indekslerin sıfırlandığından emin olun\nX_train_split = pd.DataFrame(X_train_split).reset_index(drop=True)\nX_val = pd.DataFrame(X_val).reset_index(drop=True)\ny_train_split = pd.Series(y_train_split).reset_index(drop=True)\ny_val = pd.Series(y_val).reset_index(drop=True)\n","metadata":{},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Modelin eğitimi\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\nfor epoch in range(3):  # Epoch sayısını artırabilirsiniz\n    train_acc = train_epoch(model, train_loader, optimizer, device)\n    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_acc:.4f}\")\n","metadata":{},"execution_count":34,"outputs":[{"ename":"KeyError","evalue":"3353","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 3353","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Epoch sayısını artırabilirsiniz\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, device)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     72\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     77\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[26], line 22\u001b[0m, in \u001b[0;36mNLIDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m premise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpremises[index]\n\u001b[0;32m     21\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypotheses[index]\n\u001b[1;32m---> 22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Premise ve Hypothesis metinlerini BERT için tokenleştirme\u001b[39;00m\n\u001b[0;32m     25\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     26\u001b[0m     premise,\n\u001b[0;32m     27\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m )\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 3353"]}]},{"cell_type":"code","source":"# Eğitim ve doğrulama veri setlerinin indekslerini sıfırlama\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    list(zip(train_data['premise'], train_data['hypothesis'])),\n    train_data['label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Split edilen verilerin de indekslerini sıfırlayın\nX_train_split = pd.DataFrame(X_train_split).reset_index(drop=True)\nX_val = pd.DataFrame(X_val).reset_index(drop=True)\ny_train_split = pd.Series(y_train_split).reset_index(drop=True)\ny_val = pd.Series(y_val).reset_index(drop=True)\n","metadata":{},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(f\"Train labels length: {len(y_train_split)}\")\nprint(f\"Val labels length: {len(y_val)}\")\n\n# İndeks hatası olup olmadığını kontrol edin\ntry:\n    print(f\"Accessing label at index 3353: {y_train_split[3353]}\")\nexcept KeyError as e:\n    print(f\"Error: {e}\")\n","metadata":{},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"Train labels length: 9696\n\nVal labels length: 2424\n\nAccessing label at index 3353: 1\n"}]},{"cell_type":"code","source":"class NLIDataset(Dataset):\n    def __getitem__(self, index):\n        # İndeksin mevcut olup olmadığını kontrol et\n        if index >= len(self.labels):\n            raise IndexError(f\"Index {index} out of bounds for labels of length {len(self.labels)}\")\n        \n        premise = self.premises[index]\n        hypothesis = self.hypotheses[index]\n        label = self.labels[index]\n\n        # Premise ve Hypothesis metinlerini BERT için tokenleştirme\n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'premise': premise,\n            'hypothesis': hypothesis,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding['token_type_ids'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Modeli eğitme ve doğruluk takibi\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# Cihazı kontrol etme (GPU/CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Eğitim döngüsü\nfor epoch in range(3):  # Daha fazla epoch sayısı artırılabilir\n    train_acc = train_epoch(model, train_loader, optimizer, device)\n    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_acc:.4f}\")\n\n    # Validation seti üzerinde doğruluk kontrolü\n    val_acc = validate(model, val_loader, device)\n    print(f\"Epoch {epoch + 1}, Validation Accuracy: {val_acc:.4f}\")\n","metadata":{},"execution_count":47,"outputs":[{"name":"stderr","output_type":"stream","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"},{"ename":"KeyError","evalue":"3378","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 3378","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[47], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Eğitim döngüsü\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Daha fazla epoch sayısı artırılabilir\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, device)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Validation seti üzerinde doğruluk kontrolü\u001b[39;00m\n","Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     72\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     75\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     77\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[26], line 22\u001b[0m, in \u001b[0;36mNLIDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m premise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpremises[index]\n\u001b[0;32m     21\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypotheses[index]\n\u001b[1;32m---> 22\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Premise ve Hypothesis metinlerini BERT için tokenleştirme\u001b[39;00m\n\u001b[0;32m     25\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     26\u001b[0m     premise,\n\u001b[0;32m     27\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m )\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 3378"]}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# DistilBERT tokenizer ve modelini yükleme\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n\n# Cihaz kontrolü (GPU veya CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n","metadata":{},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a028bd7efca46029da0981eff11f0b6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n\n  warnings.warn(message)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5485b0ce8e0743cdbc535cfcf4e77a71","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42900a7bfab04ef4b2a5c2a12ef9ab04","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1979f2e2f31b4488b4ec0fc2c2f34bd2","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"215c8c5eb1ba43d2a178e4b8ff02c670","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"}]},{"cell_type":"code","source":"class NLIDataset(Dataset):\n    def __init__(self, premises, hypotheses, labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.premises)\n\n    def __getitem__(self, index):\n        premise = self.premises[index]\n        hypothesis = self.hypotheses[index]\n        label = self.labels[index]\n\n        # Premise ve Hypothesis metinlerini DistilBERT için tokenleştirme\n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Eğitim ve doğrulama setlerine ayırma\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    list(zip(train_data['premise'], train_data['hypothesis'])),\n    train_data['label'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Dataset oluşturma\ntrain_dataset = NLIDataset([x[0] for x in X_train_split], [x[1] for x in X_train_split], y_train_split, tokenizer, max_len=128)\nval_dataset = NLIDataset([x[0] for x in X_val], [x[1] for x in X_val], y_val, tokenizer, max_len=128)\n\n# DataLoader ile veri yükleyici oluşturma\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n","metadata":{},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n\n# Optimizer tanımlama\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Eğitim fonksiyonu\ndef train_epoch(model, data_loader, optimizer, device):\n    model = model.train()\n    total_correct = 0\n    total_samples = 0\n\n    for batch in data_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        _, preds = torch.max(logits, dim=1)\n        total_correct += torch.sum(preds == labels)\n        total_samples += len(labels)\n\n        loss.backward()\n        optimizer.step()\n\n    accuracy = total_correct.double() / total_samples\n    return accuracy\n\n# Modeli eğitme döngüsü\nfor epoch in range(3):  # Epoch sayısını artırabilirsiniz\n    train_acc = train_epoch(model, train_loader, optimizer, device)\n    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_acc:.4f}\")\n\n    # Validation seti üzerinde doğruluk kontrolü\n    val_acc = validate(model, val_loader, device)\n    print(f\"Epoch {epoch + 1}, Validation Accuracy: {val_acc:.4f}\")\n","metadata":{},"execution_count":55,"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n  warnings.warn(\n"},{"ename":"KeyError","evalue":"5528","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 5528","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[55], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Modeli eğitme döngüsü\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Epoch sayısını artırabilirsiniz\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, device)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Validation seti üzerinde doğruluk kontrolü\u001b[39;00m\n","Cell \u001b[1;32mIn[55], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[51], line 15\u001b[0m, in \u001b[0;36mNLIDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     13\u001b[0m premise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpremises[index]\n\u001b[0;32m     14\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypotheses[index]\n\u001b[1;32m---> 15\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Premise ve Hypothesis metinlerini DistilBERT için tokenleştirme\u001b[39;00m\n\u001b[0;32m     18\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m     19\u001b[0m     premise,\n\u001b[0;32m     20\u001b[0m     hypothesis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     28\u001b[0m )\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n","File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 5528"]}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n# Premise ve Hypothesis sütunlarını birleştiriyoruz\ntrain_data['combined_text'] = train_data['premise'] + ' ' + train_data['hypothesis']\ntest_data['combined_text'] = test_data['premise'] + ' ' + test_data['hypothesis']\n\n# TF-IDF vektörleştirici\ntfidf = TfidfVectorizer(max_features=5000)  # Maksimum 5000 özellik ile sınırladık\n\n# Eğitim ve test setindeki metinleri TF-IDF ile dönüştürme\nX_train_tfidf = tfidf.fit_transform(train_data['combined_text'])\nX_test_tfidf = tfidf.transform(test_data['combined_text'])\n\n# Hedef değişkeni (label)\ny_train = train_data['label']\n","metadata":{},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n\n# Eğitim ve doğrulama setlerine ayırma\nX_train_split, X_val, y_train_split, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n\n# LightGBM modelini başlatma\nlgb_model = lgb.LGBMClassifier(random_state=42)\n\n# Modeli eğitme\nlgb_model.fit(X_train_split, y_train_split)\n\n# Doğrulama seti üzerinde tahmin yapma\ny_val_pred = lgb_model.predict(X_val)\n\n# Doğruluk skorunu hesaplama\nval_accuracy = accuracy_score(y_val, y_val_pred)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n","metadata":{},"execution_count":59,"outputs":[{"name":"stdout","output_type":"stream","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033760 seconds.\n\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 31437\n\n[LightGBM] [Info] Number of data points in the train set: 9696, number of used features: 1223\n\n[LightGBM] [Info] Start training from score -1.070244\n\n[LightGBM] [Info] Start training from score -1.138056\n\n[LightGBM] [Info] Start training from score -1.088760\n\nValidation Accuracy: 37.62%\n"}]},{"cell_type":"code","source":"# Test seti üzerinde tahmin yapma\ntest_predictions = lgb_model.predict(X_test_tfidf)\n\n# Sonuç dosyasını oluşturma\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'prediction': test_predictions\n})\n\n# Sonuçları submission.csv dosyasına kaydetme\nsubmission.to_csv('submission_lgb.csv', index=False)\nprint(\"LightGBM submission file created successfully!\")\n","metadata":{},"execution_count":61,"outputs":[{"name":"stdout","output_type":"stream","text":"LightGBM submission file created successfully!\n"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# LightGBM için parametre aralığı belirleme\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30],\n    'learning_rate': [0.01, 0.05, 0.1]\n}\n\n# GridSearchCV ile en iyi parametreleri bulma\ngrid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train_split, y_train_split)\n\n# En iyi parametreleri yazdırma\nprint(f\"Best Parameters: {grid_search.best_params_}\")\n\n# En iyi modeli seçme\nbest_model = grid_search.best_estimator_\n\n# Validation seti üzerinde en iyi modelin performansını kontrol etme\ny_val_pred_best = best_model.predict(X_val)\nbest_val_accuracy = accuracy_score(y_val, y_val_pred_best)\nprint(f\"Best Validation Accuracy: {best_val_accuracy * 100:.2f}%\")\n","metadata":{},"execution_count":63,"outputs":[{"name":"stdout","output_type":"stream","text":"Fitting 3 folds for each of 27 candidates, totalling 81 fits\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032345 seconds.\n\nYou can set `force_row_wise=true` to remove the overhead.\n\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 31437\n\n[LightGBM] [Info] Number of data points in the train set: 9696, number of used features: 1223\n\n[LightGBM] [Info] Start training from score -1.070244\n\n[LightGBM] [Info] Start training from score -1.138056\n\n[LightGBM] [Info] Start training from score -1.088760\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nBest Parameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 100}\n\nBest Validation Accuracy: 41.50%\n"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# LightGBM için daha geniş bir parametre aralığı\nparam_grid = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [10, 20, 30, 50],\n    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n    'num_leaves': [20, 30, 40],\n    'min_child_samples': [20, 30, 40],\n    'subsample': [0.6, 0.8, 1.0]\n}\n\n# GridSearchCV ile en iyi parametreleri bulma\ngrid_search = GridSearchCV(estimator=lgb.LGBMClassifier(random_state=42), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train_split, y_train_split)\n\n# En iyi parametreleri yazdırma\nprint(f\"Best Parameters: {grid_search.best_params_}\")\n\n# En iyi modeli seçme\nbest_model = grid_search.best_estimator_\n\n# Validation seti üzerinde en iyi modelin performansını kontrol etme\ny_val_pred_best = best_model.predict(X_val)\nbest_val_accuracy = accuracy_score(y_val, y_val_pred_best)\nprint(f\"Best Validation Accuracy: {best_val_accuracy * 100:.2f}%\")\n","metadata":{},"execution_count":65,"outputs":[{"name":"stdout","output_type":"stream","text":"Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036590 seconds.\n\nYou can set `force_row_wise=true` to remove the overhead.\n\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 31437\n\n[LightGBM] [Info] Number of data points in the train set: 9696, number of used features: 1223\n\n[LightGBM] [Info] Start training from score -1.070244\n\n[LightGBM] [Info] Start training from score -1.138056\n\n[LightGBM] [Info] Start training from score -1.088760\n\nBest Parameters: {'learning_rate': 0.001, 'max_depth': 30, 'min_child_samples': 20, 'n_estimators': 200, 'num_leaves': 30, 'subsample': 0.6}\n\nBest Validation Accuracy: 41.91%\n"}]},{"cell_type":"code","source":"# En iyi parametrelerle modeli yeniden eğitme\nbest_params = grid_search.best_params_\n\n# En iyi parametrelerle LightGBM modelini başlatma\nbest_lgb_model = lgb.LGBMClassifier(\n    n_estimators=best_params['n_estimators'],\n    max_depth=best_params['max_depth'],\n    learning_rate=best_params['learning_rate'],\n    num_leaves=best_params['num_leaves'],\n    min_child_samples=best_params['min_child_samples'],\n    subsample=best_params['subsample'],\n    random_state=42\n)\n\n# En iyi model ile eğitim setini yeniden eğitme\nbest_lgb_model.fit(X_train_split, y_train_split)\n\n# Validation seti üzerinde en iyi modelin performansını kontrol etme\ny_val_pred_best = best_lgb_model.predict(X_val)\nbest_val_accuracy = accuracy_score(y_val, y_val_pred_best)\nprint(f\"Validation Accuracy with Best Parameters: {best_val_accuracy * 100:.2f}%\")\n","metadata":{},"execution_count":67,"outputs":[{"name":"stdout","output_type":"stream","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037779 seconds.\n\nYou can set `force_row_wise=true` to remove the overhead.\n\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 31437\n\n[LightGBM] [Info] Number of data points in the train set: 9696, number of used features: 1223\n\n[LightGBM] [Info] Start training from score -1.070244\n\n[LightGBM] [Info] Start training from score -1.138056\n\n[LightGBM] [Info] Start training from score -1.088760\n\nValidation Accuracy with Best Parameters: 41.91%\n"}]},{"cell_type":"code","source":"# Test seti üzerinde tahmin yapma\ntest_predictions_best = best_lgb_model.predict(X_test_tfidf)\n\n# Sonuçları submission dosyasına kaydetme\nsubmission = pd.DataFrame({\n    'id': test_data['id'],\n    'prediction': test_predictions_best\n})\n\n# Sonuçları submission.csv dosyasına kaydetme\nsubmission.to_csv('submission_lgb_best.csv', index=False)\nprint(\"Submission file with best parameters created successfully!\")\n","metadata":{},"execution_count":69,"outputs":[{"name":"stdout","output_type":"stream","text":"Submission file with best parameters created successfully!\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}